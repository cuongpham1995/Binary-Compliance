---
title: "Complier distribution"
author: "Cuong Pham"
date: "2024-11-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
```

```{r}
n.pct = function(x, dec.pt = 1){
  res = paste( format(round(sum(x == 1),dec.pt), nsmall = 1) , " (", format( round( mean(x == 1)*100 ,dec.pt), nsmall = 1) , "%",")", sep = "")
  return(res)
}

mean.sd = function(x, dec.pt = 1){
  res = paste( format( round(mean(x),dec.pt), nsmall = 1) , " (", format( round(sd(x),dec.pt), nsmall = 1 ) , ")", sep = "")
  return(res)
}


w<-function(y, alpha0, alpha1){
  expit(alpha0 +alpha1*y)
}
expit<-function(x){
  exp(x)/(1+exp(x))
}

find.alpha0 = function(ps4, p1, alpha1){
  #ps4 is P(S4|A = Z = z)
  #p1 is P(Y = 1| A = Z = z)
  #pn1 is P(Y = -1| A = Z = z)
  pn1 = 1 - p1
  a = ps4
  b= ps4*exp(-alpha1) + ps4*exp(alpha1) - pn1*exp(-alpha1) - p1*exp(alpha1)
  c = ps4 - pn1 - p1
  
  #res1 = -log((-b + sqrt(b^2 -4*a*c))/(2*a))
  #res2 = -log((-b - sqrt(b^2 -4*a*c))/(2*a))
  
  res1 = suppressWarnings(-log((-b + sqrt(b^2 - 4 * a * c)) / (2 * a)))
  res2 = suppressWarnings(-log((-b - sqrt(b^2 - 4 * a * c)) / (2 * a)))
  
  res = c(res1, res2)
  return(res[!is.nan(res)] )
}

```


```{r}

dat.coc.bi = read.csv("/home/cpham/R61/datENGAGE.csv")

dat.coc.bi = dat.coc.bi

#covariate of the population distribution

#dat.popuplation.summary = dat.coc.bi %>% mutate(complier = I(trt == compl)) %>% filter(complier)

tab.sum = dat.coc.bi %>% mutate(complier = I(trt == compl)) %>% summarise(Race = n.pct(aa), Sex = n.pct(male), higheduc = mean.sd(higheduc), higheduc1 = n.pct(higheduc == 3), higheduc2 = n.pct(higheduc == 4), higheduc3 = n.pct(higheduc == 5), highedu4 = n.pct(higheduc == 6), smoke_baseline = mean.sd(smoke_baseline), Txready = mean(TxReadiness_baseline), Health = mean.sd(GH_baseline), Emo = mean.sd(emotional_baseline), trt = n.pct(trt))

tab.sum
library(xtable)
tab.sum %>% t() %>% xtable()

dat$L5[dat$gamma != 0] %>% hist()

boxplot(dat.coc.bi$emotional_baseline ~ dat.coc.bi$DaysCoc.bi)
```


```{r}


complier.cov = function(seed, alpha_n1, alpha_p1,ps4 = 0.3, col, trt.pop,  kernel.type = "linear"){
 set.seed(seed)
    require(locClass)
    require(dplyr)
    require(matrixStats)
    require(hal9001)
    require(nnet)
    require(SuperLearner)
   

   alpha0_n1 = find.alpha0(ps4 = 0.3, p1 = 0.75, alpha1 = alpha_n1)
   alpha0_p1 = find.alpha0(ps4 = 0.3, p1 = 0.71, alpha1 = alpha_p1) 
  
    dat =  as.data.frame(dat.coc.bi)
    dat = dat.coc.bi[sample(1:nrow(dat.coc.bi), nrow(dat.coc.bi), replace = T),]
   # colnames(dat) = c("aa", "male", "higheduc", "smoke_baseline", "TxReadiness_baseline", "GH_baseline", "emotional_baseline", "Z", "A", "outcome")
     colnames(dat) = c("L1", "L2", "L3", "L4", "L5", "L6", "L7", "Z", "A", "outcome")
    #dat$train = rbinom(nrow(dat), size = 1, prob = 0.7) #perform cross validation (70% train and 30% test set)
     #dat$L5 = ifelse(dat$L5 > 30,1,0)
    #calculate the weight w+ for Z = 1 and w- for Z = -1
    dat$wd = ifelse(dat$Z == -1, w(dat$outcome, alpha0_n1, alpha_n1), w(dat$outcome, alpha0_p1, alpha_p1))
    dat$wd2 = ifelse(dat$A != dat$Z, 0, dat$wd)
    
    ################# Model training #################################
    
    ##calculate the nuisance parameters
    
    #propensity score f(Z|X)
    #mod.logit  = glm(as.factor(Z) ~ L1 + L2 + L3 + L4 + L5 + L6 + L7, data = dat , family = binomial(link = logit))
    #fz = predict(mod.logit, newdata = dat, type = "response")
    #fz = ifelse(dat$Z == 1, fz, 1 - fz)
    #dat$fz = fz #propensity score
    
    #calculate f(A|Z,X) 
    #mod.multi = multinom(A ~ Z + L1 + L2 + L3 + L4 + L5 + L6 + L7, data = dat, trace = FALSE)
    #faz = predict(mod.multi,newdata = dat ,type = "prob")
    #dat$faz = ifelse(dat$Z == 1,faz[,3],faz[,1]) #f(A|Z,X).
    
    #calculate Q+ = E[yw+ | A = 1, Z = 1, L1, L2, L3]
    #Q+ and gamma+ using super learner (random forest)
    dat.pos = dat[dat$Z == 1 & dat$A == 1, ]
    y.pos = dat.pos[,"outcome"]
    y.pos = (y.pos+1)/2 
    #x.pos = dat.pos[,c("L1", "L2", "L3","L4" ,"L5", "L6", "L7"), drop = F]
    x.pos = dat.pos[,c(col), drop = F]
    sl.pos = SuperLearner(Y = y.pos, X = x.pos, family = binomial(), cvControl = list(V = 5), SL.library = c("SL.ranger" , "SL.glm") )
    #sl.pred = predict(sl.pos, dat[, c("L1", "L2", "L3","L4" ,"L5", "L6", "L7"), drop = F])$pred 
    sl.pred = predict(sl.pos, dat[, c(col), drop = F])$pred 
  
    Ey.wp = p.gammap = NA
    
    for(i in 1:nrow(dat)){
      temp.yp = rbinom(5000, 1, sl.pred[i]) #the true model for yp (y positive) has only intercept
      temp.yp = ifelse(temp.yp == 0, -1, 1)
      Ey.wp[i] = mean(temp.yp*w(temp.yp,alpha0 = alpha0_p1, alpha1 = alpha_p1)) #calculate E[Yw+| A = Z = 1, L]
      p.gammap[i] = mean(w(temp.yp, alpha0 = alpha0_p1, alpha1 = alpha_p1)) #calculate gamma+ (E[w+| A = Z = 1, L])
    }
    
    dat$Ey.wp = Ey.wp  #E[Yw+| A = Z = 1, L]
    dat$p.gammap = p.gammap #E[w+| A = Z = 1, L]
    
    #calculate Q- = E[yw- | A = -1, Z = -1, L]
    #Q- and gamma- are estimated usling super learner (random forest)
    dat.neg = dat[dat$Z == -1 & dat$A == -1, ]
    y.neg = dat.neg[,"outcome"]
    y.neg = (y.neg+1)/2 
    #x.neg = dat.neg[,c("L1", "L2", "L3","L4" ,"L5", "L6", "L7"), drop = F]
    x.neg = dat.neg[,c(col), drop = F]
    sl.neg = SuperLearner(Y = y.neg, X = x.neg, family = binomial(), cvControl = list(V = 5), SL.library = c("SL.ranger") )
    #sl.pred.neg = predict(sl.neg, dat[,c("L1", "L2", "L3","L4" ,"L5", "L6", "L7"), drop = F])$pred 
    sl.pred.neg = predict(sl.neg, dat[,c(col), drop = F])$pred 
    
    Ey.wn = p.gamman = NA
    
    for(i in 1:nrow(dat)){
      temp.yn = rbinom(5000, 1, sl.pred.neg)
      temp.yn = ifelse(temp.yn == 0, -1, 1)
      Ey.wn[i] = mean(temp.yn*w(temp.yn,alpha0 = alpha0_n1, alpha1 = alpha_n1)) #calculate E[Yw-|A = Z = -1, L]
      p.gamman[i] = mean(w(temp.yn, alpha0 = alpha0_n1, alpha1 = alpha_n1)) #calculate gamma- (E[w-|A= Z = -1, L]) 
    }
    
    dat$Ey.wn = Ey.wn
    dat$p.gamman = p.gamman
    dat$gamma = ifelse(dat$Z == 1, p.gammap, p.gamman)
    dat$gamma = ifelse(dat$Z != dat$A, 0, dat$gamma)
    
    
  #  temp.dat = dat %>% filter(dat[,col] == 1)
  #  p.1 = mean(temp.dat$A == temp.dat$Z) #f(A = Z | X= 1 )
    
   # temp.dat = dat %>% filter(dat[,col] == 0)
  #  p.0 = mean(temp.dat$A == temp.dat$Z) #f(A = Z | X = 0)
    
  #  dat$f.a.z.x = ifelse(dat[,col] == 1, p.1, p.0)
    mu.S4 = mean(dat$gamma[dat$gamma != 0 & dat$A == trt.pop])    
   # mean(dat[dat$gamma != 0,col])
  #  mean(dat[dat$gamma != 0,col]*(dat$gamma[dat$gamma != 0]/mu.S4))
   # return(list(complier = mean(dat[dat$gamma != 0,col]*(dat$gamma[dat$gamma != 0]/mu.S4)), 
   #             population = mean(dat[,col]) )
 # return(list(population =mean(dat[,col]),
  #            complier = mean(dat[dat$gamma != 0,col]*(dat$gamma[dat$gamma != 0]/mu.S4)) ) )
 
     return(list(comp.mean = mean(dat[dat$gamma != 0 & dat$A == trt.pop,col]*(dat$gamma[dat$gamma != 0 & dat$A == trt.pop]/mu.S4)),
                 comp.sd = sd(dat[dat$gamma != 0 & dat$A == trt.pop,col]*(dat$gamma[dat$gamma != 0 & dat$A == trt.pop]/mu.S4)) ))   
}
    

complier.cov(seed = 22, alpha_n1 = 0.5, alpha_p1 = 0.5, ps4 = 0.3, col = "L1", trt.pop = -1)  
```



```{r}
#L1 = race
#L2 = sex
#L3 = education
#L4 = smoke
#L5 = treatment ready
#L6 = general health
#L7 = Emotional baseline

#creating table 5 in the paper

alpha.grid = expand.grid(alpha_n = c(0.0000, 0.5, 1,2), alpha_p = c(0.0000, 0.5, 1,2))
alpha.grid = alpha.grid[c(6,11,16),]

#boxplot(dat.coc.bi$emotional_baseline[dat.coc.bi$DaysCoc.bi == -1])
n.sim = 100

covarites = c("L1", "L2", "L3", "L4", "L5", "L6", "L7")
tab.full = matrix(nrow = length(covarites), ncol = nrow(alpha.grid))

for(j in 1:length(covarites)){
  print(j)
L1.mean = matrix(nrow = n.sim, ncol = nrow(alpha.grid))
L1.sd = L1.mean 

for(i in 1:n.sim){
  #print(i)
  temp = mapply(seed = i , complier.cov, alpha_n1 = alpha.grid$alpha_n, alpha_p1 = alpha.grid$alpha_p, col = covarites[j], trt.pop = 1)
  
  L1.mean[i,] = unlist(temp[1,])
  L1.sd[i,] = unlist(temp[2,])
}

tab.full[j,] = paste0( round(apply(L1.mean,2,mean),2) , "(", round(apply(L1.sd,2,mean),2),")", sep = "")
}



```


```{r}

population = dat.coc.bi %>% mutate(complier = I(trt == compl)) %>% filter(complier == T & trt == 1) %>% summarise(Race = mean.sd(aa), Sex = mean.sd(male), higheduc = mean.sd(higheduc),  smoke_baseline = mean.sd(smoke_baseline), Txready = mean.sd(TxReadiness_baseline), Health = mean.sd(GH_baseline), Emo = mean.sd(emotional_baseline))

tab = as.data.frame(tab.full)

tab$population = t(population)

tab$covariates = c("Race", "Sex", "Education", "Smoke", "Treatment Readiness", "General Health", "Emotional Readiness")
tab = tab[,c(5,1:4)]

tab$Prevalence = NA
#creating table 5
tab[,c(1,2,5,6)] %>% xtable()

#supplementary table
tab[,c(1,3:5)] %>% xtable()

```

```{r}

tab.full2 = matrix(nrow = length(covarites), ncol = nrow(alpha.grid))

for(j in 1:length(covarites)){
  print(j)
L1.mean = matrix(nrow = n.sim, ncol = nrow(alpha.grid))
L1.sd = L1.mean 

for(i in 1:n.sim){
  #print(i)
  temp = mapply(seed = i , complier.cov, alpha_n1 = alpha.grid$alpha_n, alpha_p1 = alpha.grid$alpha_p, col = covarites[j], trt.pop = -1)
  
  L1.mean[i,] = unlist(temp[1,])
  L1.sd[i,] = unlist(temp[2,])
}

tab.full2[j,] = paste0( round(apply(L1.mean,2,mean),2) , "(", round(apply(L1.sd,2,mean),2),")", sep = "")
}
```

```{r}
population.An1 = dat.coc.bi %>% mutate(complier = I(trt == compl)) %>% filter(complier == T & trt == -1) %>% summarise(Race = mean.sd(aa), Sex = mean.sd(male), higheduc = mean.sd(higheduc),  smoke_baseline = mean.sd(smoke_baseline), Txready = mean.sd(TxReadiness_baseline), Health = mean.sd(GH_baseline), Emo = mean.sd(emotional_baseline))

tab2 = as.data.frame(tab.full2)

tab2$population = t(population.An1)

tab2$covariates = c("Race", "Sex", "Education", "Smoke", "Treatment Readiness", "General Health", "Emotional Readiness")
tab2 = tab2[,c(5,1:4)]

tab2$Prevalence = NA
#creating table 6
tab2[,c(1,2,5,6)] %>% xtable()

tab2[,c(1,3:5)] %>% xtable()
```



